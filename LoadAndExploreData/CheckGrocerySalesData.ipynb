{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #1 stores.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfStores = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/stores.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfStores.count()\n",
    "#54 records are present when opened in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+----------+----+-----------------+\n",
      "|summary|         store_nbr|         city|     state|type|          cluster|\n",
      "+-------+------------------+-------------+----------+----+-----------------+\n",
      "|  count|                54|           54|        54|  54|               54|\n",
      "|   mean|              27.5|         null|      null|null|8.481481481481481|\n",
      "| stddev|15.732132722552274|         null|      null|null|4.693394558396158|\n",
      "|    min|                 1|       Ambato|     Azuay|   A|                1|\n",
      "|    max|                54|Santo Domingo|Tungurahua|   E|               17|\n",
      "+-------+------------------+-------------+----------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfStores.describe().show()\n",
    "#Count matches as seen below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cluster: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfStores.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# First 5 rows by opening the file in windows:\n",
    "# 1,Quito,Pichincha,D,13\n",
    "# 2,Quito,Pichincha,D,13\n",
    "# 3,Quito,Pichincha,D,8\n",
    "# 4,Quito,Pichincha,D,9\n",
    "# 5,Santo Domingo,Santo Domingo de los Tsachilas,D,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfStores.show(5))\n",
    "\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Last 5 rows by opening the file in windows:\n",
    "49,Quito,Pichincha,A,11\n",
    "50,Ambato,Tungurahua,A,14\n",
    "51,Guayaquil,Guayas,A,17\n",
    "52,Manta,Manabi,A,11\n",
    "53,Manta,Manabi,D,13\n",
    "54,El Carmen,Manabi,C,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----+-------+\n",
      "|store_nbr|     city|     state|type|cluster|\n",
      "+---------+---------+----------+----+-------+\n",
      "|       49|    Quito| Pichincha|   A|     11|\n",
      "|       50|   Ambato|Tungurahua|   A|     14|\n",
      "|       51|Guayaquil|    Guayas|   A|     17|\n",
      "|       52|    Manta|    Manabi|   A|     11|\n",
      "|       53|    Manta|    Manabi|   D|     13|\n",
      "|       54|El Carmen|    Manabi|   C|      3|\n",
      "+---------+---------+----------+----+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfStores.filter(sdfStores.store_nbr >= 49).show())\n",
    "\n",
    "# Last 5 lines in windows are present in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #2 oil.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfOil = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/oil.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfOil.count()\n",
    "#1218 records are present when opened in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- dcoilwtico: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfOil.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First 5 rows by opening the file in windows:\n",
    "2013-01-01,\n",
    "2013-01-02,93.14\n",
    "2013-01-03,92.97\n",
    "2013-01-04,93.12\n",
    "2013-01-07,93.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|dcoilwtico|\n",
      "+-------------------+----------+\n",
      "|2013-01-01 00:00:00|      null|\n",
      "|2013-01-02 00:00:00|     93.14|\n",
      "|2013-01-03 00:00:00|     92.97|\n",
      "|2013-01-04 00:00:00|     93.12|\n",
      "|2013-01-07 00:00:00|      93.2|\n",
      "+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "\n",
    "print(sdfOil.show(5))\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #3 holidays_events.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfHolidayEvents = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/holidays_events.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfHolidayEvents.count()\n",
    "#350 records are present when opened in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+--------------------+--------------------+\n",
      "|summary|      type|  locale|         locale_name|         description|\n",
      "+-------+----------+--------+--------------------+--------------------+\n",
      "|  count|       350|     350|                 350|                 350|\n",
      "|   mean|      null|    null|                null|                null|\n",
      "| stddev|      null|    null|                null|                null|\n",
      "|    min|Additional|   Local|              Ambato|Batalla de Pichincha|\n",
      "|    max|  Work Day|Regional|Santo Domingo de ...|       Viernes Santo|\n",
      "+-------+----------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfHolidayEvents.describe().show()\n",
    "#Count matches as seen below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- locale: string (nullable = true)\n",
      " |-- locale_name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- transferred: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfHolidayEvents.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First 5 rows by opening the file in windows:\n",
    "2012-03-02,Holiday,Local,Manta,Fundacion de Manta,False\n",
    "2012-04-01,Holiday,Regional,Cotopaxi,Provincializacion de Cotopaxi,False\n",
    "2012-04-12,Holiday,Local,Cuenca,Fundacion de Cuenca,False\n",
    "2012-04-14,Holiday,Local,Libertad,Cantonizacion de Libertad,False\n",
    "2012-04-21,Holiday,Local,Riobamba,Cantonizacion de Riobamba,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|               date|   type|  locale|locale_name|         description|transferred|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|2012-03-02 00:00:00|Holiday|   Local|      Manta|  Fundacion de Manta|      false|\n",
      "|2012-04-01 00:00:00|Holiday|Regional|   Cotopaxi|Provincializacion...|      false|\n",
      "|2012-04-12 00:00:00|Holiday|   Local|     Cuenca| Fundacion de Cuenca|      false|\n",
      "|2012-04-14 00:00:00|Holiday|   Local|   Libertad|Cantonizacion de ...|      false|\n",
      "|2012-04-21 00:00:00|Holiday|   Local|   Riobamba|Cantonizacion de ...|      false|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfHolidayEvents.show(5))\n",
    "\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #4 items.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfItems = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/items.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfItems.count()\n",
    "#4100 records are present when opened in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+---------------+-------------------+\n",
      "|summary|         item_nbr|    family|          class|         perishable|\n",
      "+-------+-----------------+----------+---------------+-------------------+\n",
      "|  count|             4100|      4100|           4100|               4100|\n",
      "|   mean|1251436.312682927|      null|        2169.65|0.24048780487804877|\n",
      "| stddev|587687.2203100388|      null|1484.9109001053|0.42743184376713605|\n",
      "|    min|            96995|AUTOMOTIVE|           1002|                  0|\n",
      "|    max|          2134244|   SEAFOOD|           7780|                  1|\n",
      "+-------+-----------------+----------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfItems.describe().show()\n",
    "#Count matches as seen below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_nbr: integer (nullable = true)\n",
      " |-- family: string (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      " |-- perishable: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfItems.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First 5 rows by opening the file in windows:\n",
    "96995,GROCERY I,1093,0\n",
    "99197,GROCERY I,1067,0\n",
    "103501,CLEANING,3008,0\n",
    "103520,GROCERY I,1028,0\n",
    "103665,BREAD/BAKERY,2712,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----+----------+\n",
      "|item_nbr|      family|class|perishable|\n",
      "+--------+------------+-----+----------+\n",
      "|   96995|   GROCERY I| 1093|         0|\n",
      "|   99197|   GROCERY I| 1067|         0|\n",
      "|  103501|    CLEANING| 3008|         0|\n",
      "|  103520|   GROCERY I| 1028|         0|\n",
      "|  103665|BREAD/BAKERY| 2712|         1|\n",
      "+--------+------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfItems.show(5))\n",
    "\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Last 5 rows by opening the file in windows:\n",
    "2132318,GROCERY I,1002,0\n",
    "2132945,GROCERY I,1026,0\n",
    "2132957,GROCERY I,1068,0\n",
    "2134058,BEVERAGES,1124,0\n",
    "2134244,\"LIQUOR,WINE,BEER\",1364,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----+----------+\n",
      "|item_nbr|          family|class|perishable|\n",
      "+--------+----------------+-----+----------+\n",
      "| 2132318|       GROCERY I| 1002|         0|\n",
      "| 2132945|       GROCERY I| 1026|         0|\n",
      "| 2132957|       GROCERY I| 1068|         0|\n",
      "| 2134058|       BEVERAGES| 1124|         0|\n",
      "| 2134244|LIQUOR,WINE,BEER| 1364|         0|\n",
      "+--------+----------------+-----+----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfItems.filter(sdfItems.item_nbr >= 2132318).show())\n",
    "\n",
    "# Last 5 lines in windows are present in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #5 transactions.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfTransactions = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/transactions.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83488"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfTransactions.count()\n",
    "#83488 records are present when opened in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|         store_nbr|      transactions|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             83488|             83488|\n",
      "|   mean|26.939236776542735|1694.6021583940208|\n",
      "| stddev|15.608203645366135| 963.2866435167193|\n",
      "|    min|                 1|                 5|\n",
      "|    max|                54|              8359|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfTransactions.describe().show()\n",
    "#Count matches as seen below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- transactions: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfTransactions.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First 5 rows by opening the file in windows:\n",
    "2013-01-01,25,770\n",
    "2013-01-02,1,2111\n",
    "2013-01-02,2,2358\n",
    "2013-01-02,3,3487\n",
    "2013-01-02,4,1922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------------+\n",
      "|               date|store_nbr|transactions|\n",
      "+-------------------+---------+------------+\n",
      "|2013-01-01 00:00:00|       25|         770|\n",
      "|2013-01-02 00:00:00|        1|        2111|\n",
      "|2013-01-02 00:00:00|        2|        2358|\n",
      "|2013-01-02 00:00:00|        3|        3487|\n",
      "|2013-01-02 00:00:00|        4|        1922|\n",
      "+-------------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfTransactions.show(5))\n",
    "\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #6 sample_submission.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfSampleSubmission = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/sample_submission.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3370464"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfSampleSubmission.count()\n",
    "#3370464 records are present when opened in Notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+\n",
      "|summary|               id|unit_sales|\n",
      "+-------+-----------------+----------+\n",
      "|  count|          3370464|   3370464|\n",
      "|   mean|    1.271822715E8|       0.0|\n",
      "| stddev|972969.2931845216|       0.0|\n",
      "|    min|        125497040|         0|\n",
      "|    max|        128867503|         0|\n",
      "+-------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfSampleSubmission.describe().show()\n",
    "#Count matches as seen below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- unit_sales: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfSampleSubmission.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First 5 rows by opening the file in windows:\n",
    "125497040,0\n",
    "125497041,0\n",
    "125497042,0\n",
    "125497043,0\n",
    "125497044,0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|       id|unit_sales|\n",
      "+---------+----------+\n",
      "|125497040|         0|\n",
      "|125497041|         0|\n",
      "|125497042|         0|\n",
      "|125497043|         0|\n",
      "|125497044|         0|\n",
      "+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfSampleSubmission.show(5))\n",
    "\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Last 5 rows by opening the file in windows:\n",
    "128867499,0\n",
    "128867500,0\n",
    "128867501,0\n",
    "128867502,0\n",
    "128867503,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|       id|unit_sales|\n",
      "+---------+----------+\n",
      "|128867499|         0|\n",
      "|128867500|         0|\n",
      "|128867501|         0|\n",
      "|128867502|         0|\n",
      "|128867503|         0|\n",
      "+---------+----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfSampleSubmission.filter(sdfSampleSubmission.id >= 128867499).show())\n",
    "\n",
    "# Last 5 lines in windows are present in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #7 test.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfTest = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/test.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3370464"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfTest.count()\n",
    "#3370464 records are present when opened in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------------------+------------------+\n",
      "|summary|              id|         store_nbr|          item_nbr|\n",
      "+-------+----------------+------------------+------------------+\n",
      "|  count|         3370464|           3370464|           3370464|\n",
      "|   mean|   1.271822715E8|              27.5|1244798.2476288131|\n",
      "| stddev|972969.293184528|15.585786433120868| 589836.2185152168|\n",
      "|    min|       125497040|                 1|             96995|\n",
      "|    max|       128867503|                54|           2134244|\n",
      "+-------+----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfTest.describe().show()\n",
    "#Count matches as seen below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- item_nbr: integer (nullable = true)\n",
      " |-- onpromotion: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfTest.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First 5 rows by opening the file in windows:\n",
    "125497040,2017-08-16,1,96995,False\n",
    "125497041,2017-08-16,1,99197,False\n",
    "125497042,2017-08-16,1,103501,False\n",
    "125497043,2017-08-16,1,103520,False\n",
    "125497044,2017-08-16,1,103665,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+-----------+\n",
      "|       id|               date|store_nbr|item_nbr|onpromotion|\n",
      "+---------+-------------------+---------+--------+-----------+\n",
      "|125497040|2017-08-16 00:00:00|        1|   96995|      false|\n",
      "|125497041|2017-08-16 00:00:00|        1|   99197|      false|\n",
      "|125497042|2017-08-16 00:00:00|        1|  103501|      false|\n",
      "|125497043|2017-08-16 00:00:00|        1|  103520|      false|\n",
      "|125497044|2017-08-16 00:00:00|        1|  103665|      false|\n",
      "+---------+-------------------+---------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfTest.show(5))\n",
    "\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Last 5 rows by opening the file in windows:\n",
    "128867499,2017-08-31,54,2132163,False\n",
    "128867500,2017-08-31,54,2132318,False\n",
    "128867501,2017-08-31,54,2132945,False\n",
    "128867502,2017-08-31,54,2132957,False\n",
    "128867503,2017-08-31,54,2134244,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+-----------+\n",
      "|       id|               date|store_nbr|item_nbr|onpromotion|\n",
      "+---------+-------------------+---------+--------+-----------+\n",
      "|128867499|2017-08-31 00:00:00|       54| 2132163|      false|\n",
      "|128867500|2017-08-31 00:00:00|       54| 2132318|      false|\n",
      "|128867501|2017-08-31 00:00:00|       54| 2132945|      false|\n",
      "|128867502|2017-08-31 00:00:00|       54| 2132957|      false|\n",
      "|128867503|2017-08-31 00:00:00|       54| 2134244|      false|\n",
      "+---------+-------------------+---------+--------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfTest.filter(sdfTest.id >= 128867499).show())\n",
    "\n",
    "# Last 5 lines in windows are present in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check file #8 train.csv</b> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " sdfTrain = sqlContext.read.csv(\"/gl-capstone-data/Team6-C-Sep/Data/train/train.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125497040"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of lines by opening the file in windows using Powershell (excl header): 125497040\n",
    "# Count of records in the dataframe                                           : 125497040\n",
    "# The entire file has been successfully uploaded\n",
    "\n",
    "sdfTrain.count()\n",
    "#Count before 13-May: 49391194 (Entire file did not get uploaded)\n",
    "#Count on 19-May: 125497040 (Entire file has been uploaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfTrain.createOrReplaceTempView(\"vwTrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-----------------+------------------+\n",
      "|summary|                 id|         store_nbr|         item_nbr|        unit_sales|\n",
      "+-------+-------------------+------------------+-----------------+------------------+\n",
      "|  count|          125497040|         125497040|        125497040|         125497040|\n",
      "|   mean|       6.27485195E7|27.464578025107205| 972769.152722598| 8.554865268438212|\n",
      "| stddev|3.622787505758819E7| 16.33051030272046|520533.5986107415|23.605151809802148|\n",
      "|    min|                  0|                 1|            96995|          -15372.0|\n",
      "|    max|          125497039|                54|          2127114|           89440.0|\n",
      "+-------+-------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfTrain.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- store_nbr: integer (nullable = true)\n",
      " |-- item_nbr: integer (nullable = true)\n",
      " |-- unit_sales: double (nullable = true)\n",
      " |-- onpromotion: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfTrain.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------+----------+-----------+\n",
      "| id|               date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
      "+---+-------------------+---------+--------+----------+-----------+\n",
      "|  0|2013-01-01 00:00:00|       25|  103665|       7.0|       null|\n",
      "|  1|2013-01-01 00:00:00|       25|  105574|       1.0|       null|\n",
      "|  2|2013-01-01 00:00:00|       25|  105575|       2.0|       null|\n",
      "|  3|2013-01-01 00:00:00|       25|  108079|       1.0|       null|\n",
      "|  4|2013-01-01 00:00:00|       25|  108701|       1.0|       null|\n",
      "+---+-------------------+---------+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# First 5 rows by opening the file in windows:\n",
    "# 0,2013-01-01,25,103665,7.0,\n",
    "# 1,2013-01-01,25,105574,1.0,\n",
    "# 2,2013-01-01,25,105575,2.0,\n",
    "# 3,2013-01-01,25,108079,1.0,\n",
    "# 4,2013-01-01,25,108701,1.0,\n",
    "\n",
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfTrain.show(5))\n",
    "\n",
    "# First 5 lines are matching in windows and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+----------+-----------+\n",
      "|       id|               date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
      "+---------+-------------------+---------+--------+----------+-----------+\n",
      "|125497035|2017-08-15 00:00:00|       54| 2089339|       4.0|      false|\n",
      "|125497036|2017-08-15 00:00:00|       54| 2106464|       1.0|       true|\n",
      "|125497037|2017-08-15 00:00:00|       54| 2110456|     192.0|      false|\n",
      "|125497038|2017-08-15 00:00:00|       54| 2113914|     198.0|       true|\n",
      "|125497039|2017-08-15 00:00:00|       54| 2116416|       2.0|      false|\n",
      "+---------+-------------------+---------+--------+----------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Last 5 rows by opening the file in windows:\n",
    "# 125497035,2017-08-15,54,2089339,4.0,False\n",
    "# 125497036,2017-08-15,54,2106464,1.0,True\n",
    "# 125497037,2017-08-15,54,2110456,192.0,False\n",
    "# 125497038,2017-08-15,54,2113914,198.0,True\n",
    "# 125497039,2017-08-15,54,2116416,2.0,False\n",
    "\n",
    "# Verify that the same records are present in the file in HDFS:\n",
    "print(sdfTrain.filter(sdfTrain.id >= 125497035).show())\n",
    "\n",
    "# The last records which are present in windows are not present in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| max(id)|\n",
      "+--------+\n",
      "|49391192|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql('select max(id) from vwTrain').show()\n",
    "# Max id in windows is 125497039\n",
    "# Max id in HDFS    is  49391192 which is far less than what is present in windows\n",
    "# So it can be seen that the full file has not been uploaded to HDFS due to space constraints\n",
    "# We have requested admin to increase the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
